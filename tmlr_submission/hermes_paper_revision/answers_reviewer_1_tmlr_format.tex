\documentclass[10pt]{article} % For LaTeX2e
\usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
%\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{amssymb,amsmath,amsthm}
\usepackage[pdftex]{graphicx}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{01}  % Insert correct month for camera-ready version
\def\year{2023} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\textcolor{orange}{j'ai coupé la réponse en trois pour pouvoir respecter la limite de 5000 mots. L'endroit ou je peux ecrire la réponse ne compile pas les tableaux en latex par contre, je les ai donc enlevé. Pareil pour les citations que j'ai écrit en dur.}
\bigskip

\textbf{Response to review (first part)}.\vspace{0.2cm}

We would like to thank the reviewer for the appreciation of the paper and the constructive feedback. We have carefully considered all comments and we will propose a revision of the paper accordingly when all reviews will be published. In the meantime, you can find below responses to some of the remarks:


- We acknowledge that we did not discuss enough the choice of per-time-series models in the paper. This comment has led to additional justifications and discussions, in particular we discuss the use of state space models. We highlight that the main limitation for this choice is the computational time required to train the per-time-series models. In addition, in the experiment section, a third HERMES variation is added using Thetam as the per-times-series model.
	
- Concerning the choice of per-times-series model, the main limitation is the computational time required to train these models, as the dataset contains thousands of time series. For instance, training a HMM with the EM algorithm, or training ARIMA on thousands of time series would increase significantly the computational cost. Concerning TBATS, Exponential smoothing and Thetam, correctly fitting them with existing Python packages is possible in an reasonable time, from a few minutes for exponential smoothing models to a couple of hours for TBATS. We believe that our experiment section, in particular with the additional simulations, highlights the flexibility of the hybrid approach and the numerical results motivate the use of TBATS eventhough other models could be used. To clarify this point in the paper, several justifications and comments were added in the text.
	
- Your third remark has led to the addition of 2 new methods as benchmarks. First, the Prophet method introduced in Taylor $\&$ Letham (2017) is trained and evaluated on the fashion dataset using the Python package 'prophet'. We show that it leads to poor results in comparison to our model on the whole fashion dataset, and a comparable accuracy on the use case where only 100 time series are accessible. Secondly, the recent DeepAR method introduced in Salinas et al. (2020) is added  and trained with the Python package 'Gluonts' (Alexandrov et al., 2020). This method shows striking results on the fashion dataset and is a strong competitor of the proposed hybrid approach.

Citation:

Alexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus, Tim Januschowski, Danielle C. Maddix, Syama Rangapuram, David Salinas, Jasper Schulz, Lorenzo Stella, Ali Caner Türkmen, and Yuyang Wang. GluonTS: Probabilistic and Neural Time Series Modeling in Python. Journal of Machine Learning Research, 21(116):1–6, 2020. URL http://jmlr.org/papers/v21/
19-820.html.

David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. DeepAR: Probabilistic forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3):1181–1191, 2020.

Sean Taylor and Benjamin Letham. Forecasting at scale. The American Statistician, 72, 09 2017. doi:10.1080/00031305.2017.1380080.


\textbf{Response to review (second part)}.\vspace{0.2cm}

	
- We thank the referee for the remark concerning the analysis of diffrent types of time series. This comment led to a new part in the appendix where 4 sub-samples of the Fashion dataset are built and forecasted separately.
Their are defined as follows:
	- disrupting time series: In retail and fashion industries, a strategic issue is to correctly anticipate new trends that will burst or collapse in the next weeks, months or years. To detect these sub sample of trends in the Fashion dataset, the following approach is proposed: Using the 'snaive' model, a prediction of the last year and its linked MASE is computed for each trend. Finally, the dirsupting time series are the 1000 time series with the highest MASE.
	-stable time series: By contrast, a group a stable trend a presented. To define them, the same methodology as the previous group is used. We define them as the time series where the snaive prediction reachs the lowest MASE. 
	-seasonal time series: Another relevant group of time series is sequences showing a strong seasonal patern. So as to detect them, we compute for each trend the seasonality strength metric introduced in Wang et al. (2006). The seasonal time series group is the 1000 fashion time series with highest seasonality strength.
	-noisy time series: Finally, several time series of the Fashion dataset represent niche trends only worn and posted on social media by a few people. The average volume for these sequences is low resulting in sporadic and noisy time series difficult to forecast. In order to detect them, we use the seasonality and trend strenght metrics presented in Wang et al. (2006). For each time series, the average of these two quantities is computed and we define the time series group as the 1000 sequences with the lowest averages.
We added the predictions of each model on these sub-samples and extended the analysis of the hybrid approach, the fashion dataset and the use of external signals. A table showing the results of each method on the 4 sub-samples will be added in appendix as well as a section describing more precisely how the 4 samples are built.

Citation:

Xiaozhe Wang, Kate Smith, and Rob Hyndman. Characteristic-based clustering for time series data. Data mining and knowledge Discovery, 13:335–364, 2006.

\textbf{Response to review (third part)}.\vspace{0.2cm}

- The point concerning the impact of the proposed work on the society and environement is now developed in the discussion section. We discuss the impact of training deep architectures and the computational cost of the proposed approach. However, we highlight that the aim of this work is to show that it is possible to better forecast and anticipate consumers behaviours in the retail and fashion industry. With a better anticipation of consumer needs, companies could better adjust their production, reduce massive wastes due to their overstocks and finally offset the environmental cost of this work.
	
- We thank the referee for the remark pointing out privacy issues of using social media images. Several information concerning this issue were added in Section 2. The whole creation of the dataset respects the privacy and data protection regulations, including the General Data Protection Regulation. Concerning the image recovery on social media, only public accounts have been analysed and no potential private information were used, saved or revealed during the whole process. Concerning the time series, all the trends names have been anonymized and only macro information such as the geolocalisation, the type of cloth and the gender are revealed publicly. Concerning the time series, as they represent aggregate and normalized signals, no users' personal information can be tracked down.
	
- Concerning the potential risks or biases of using social media data, we totally agree with the referee. First of all, an important part of this work was to design a correct normalization process to remove the most visible biases brought by social media. Then, by proposing time series splitted by geo zone, we have tried to propose an accurate representation of the diversity of behaviours that exists in each region. We acknowledge that the proposed geozones may seem large. A future work could be to built trends at country level. Finally, by proposing a global model to forecast all the fashion trends, it is possible that rare patterns may not be correctly learned by the proposed framework. A future relevant work would be to learn models with latent states and an unsupervised learning



\end{document}
