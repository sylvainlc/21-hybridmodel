\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{model5-names}
\citation{makridakis2018m4}
\citation{sarkka2013bayesian,douc2014nonlinear,zucchini2017hidden}
\citation{RePEc:inm:oropre:v:9:y:1961:i:5:p:673-685}
\citation{doi:10.1198/jasa.2011.tm09771}
\citation{box2015time}
\citation{touron2017modeling}
\citation{touron2019consistency}
\citation{hochreiter1997long,vaswani2017attention,8614252,li2019enhancing,lim2019temporal,salinas2020deepar}
\citation{salinas2020deepar}
\citation{li2019enhancing}
\citation{lim2019temporal}
\citation{martin2020monte}
\citation{makridakis2018m4}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{2}{Introduction}{section.1}{}}
\citation{zhang2003time,jianwei2019novel,bandara2020lstm}
\citation{smyl2020hybrid}
\citation{makridakis2020m4}
\citation{smyl2020hybrid}
\citation{smyl2020hybrid}
\citation{doi:10.1198/jasa.2011.tm09771}
\@writefile{toc}{\contentsline {section}{\numberline {2}Hybrid model with external signals}{5}{section.2}\protected@file@percent }
\newlabel{sec:hybrid}{{2}{5}{Hybrid model with external signals}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Per-time-series predictors}{5}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Hermes forecast examples. In green the prediction of the TBATS per-time-series predictors. In red the final forecast of our HERMES hybrid model. Time series representing the vertical stipes texture fashion trend for females in Brazil.}}{6}{figure.1}\protected@file@percent }
\newlabel{fig:introexamples}{{1}{6}{Hermes forecast examples. In green the prediction of the TBATS per-time-series predictors. In red the final forecast of our HERMES hybrid model. Time series representing the vertical stipes texture fashion trend for females in Brazil}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Error-corrector recurrent model}{7}{subsection.2.2}\protected@file@percent }
\newlabel{eq:nows:full:model}{{1}{7}{Error-corrector recurrent model}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Weak signal}{7}{subsection.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Architecture of the hybrid model with weak signals.}}{8}{figure.2}\protected@file@percent }
\newlabel{fig:architecture}{{2}{8}{Architecture of the hybrid model with weak signals}{figure.2}{}}
\newlabel{eq:withws:full:model}{{2}{8}{Weak signal}{equation.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Fashion dataset with external weak signals}{8}{section.3}\protected@file@percent }
\newlabel{sec:dataset}{{3}{8}{Fashion dataset with external weak signals}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Translate fashion to data}{8}{subsection.3.1}\protected@file@percent }
\newlabel{sec:dataset:a}{{3.1}{8}{Translate fashion to data}{subsection.3.1}{}}
\citation{rogersdiffusion}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Fashion dataset}{9}{subsection.3.2}\protected@file@percent }
\newlabel{sec:dataset:b}{{3.2}{9}{Fashion dataset}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Weak signal}{9}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Example of difference between the raw sequence and the normalized one. In this example, we normalize by the deseasonalized global top fashion trend for females in China. (Top) Time series representing the raw signal of a top fashion trend for females in China. (Bottom) Time series representing the normalized signal of a top fashion trend for females in China.}}{10}{figure.3}\protected@file@percent }
\newlabel{fig:normalization}{{3}{10}{Example of difference between the raw sequence and the normalized one. In this example, we normalize by the deseasonalized global top fashion trend for females in China. (Top) Time series representing the raw signal of a top fashion trend for females in China. (Bottom) Time series representing the normalized signal of a top fashion trend for females in China}{figure.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Fashion time series overview. For each couple geozone/category, the table gives the number of trends (Female/Male).}}{10}{table.1}\protected@file@percent }
\newlabel{tab:fashiondataset}{{1}{10}{Fashion time series overview. For each couple geozone/category, the table gives the number of trends (Female/Male)}{table.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A shoes trend of the fashion dataset. In black the main signal and in red its associated \textit  {fashion-forward} weak signal. The shift between these two signals at the end of 2017/beginning of 2018 announces the future burst of the trend.}}{11}{figure.4}\protected@file@percent }
\newlabel{fig:oneemergingtrend}{{4}{11}{A shoes trend of the fashion dataset. In black the main signal and in red its associated \textit {fashion-forward} weak signal. The shift between these two signals at the end of 2017/beginning of 2018 announces the future burst of the trend}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental results}{11}{section.4}\protected@file@percent }
\newlabel{sec:exp}{{4}{11}{Experimental results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Training}{11}{subsection.4.1}\protected@file@percent }
\citation{smyl2020hybrid}
\citation{doi:10.1198/jasa.2011.tm09771}
\citation{hyndman2020package}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Temporal split for our training process. The three first years define our training set. The fourth year is used as our eval set and the final year is reserved for the test set.}}{13}{figure.5}\protected@file@percent }
\newlabel{fig:train_eval_test_set}{{5}{13}{Temporal split for our training process. The three first years define our training set. The fourth year is used as our eval set and the final year is reserved for the test set}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Architecture of the RNN corrector part of the HERMES framework. The same architecture is used in the \textit  {lstm} benchmark model.}}{13}{figure.6}\protected@file@percent }
\newlabel{fig:rnn_architecture}{{6}{13}{Architecture of the RNN corrector part of the HERMES framework. The same architecture is used in the \textit {lstm} benchmark model}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Benchmarks, hybrid models and Metrics}{14}{subsection.4.2}\protected@file@percent }
\citation{smyl2020hybrid}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Result for the Fashion dataset}{15}{subsection.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Results summary on the 10000ts Fashion dataset. For each metric, the average on all our time series is computed. For approaches using neural networks, 10 models are trained with different seeds. The mean and the standard deviation of the 10 results are displayed.}}{16}{table.2}\protected@file@percent }
\newlabel{tab:metricresults}{{2}{16}{Results summary on the 10000ts Fashion dataset. For each metric, the average on all our time series is computed. For approaches using neural networks, 10 models are trained with different seeds. The mean and the standard deviation of the 10 results are displayed}{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textit  {hermes-tbats} forecast examples. In green the prediction of the per-time-series predictors \textit  {tbats}. In red the final forecast of our HERMES hybrid model \textit  {hermes-tbats}. (Top) Time series representing a top fashion trend for females in The United States. (Bottom) Time series representing the horizontal stipes texture fashion trend for females in China.}}{17}{figure.7}\protected@file@percent }
\newlabel{fig:examples}{{7}{17}{\textit {hermes-tbats} forecast examples. In green the prediction of the per-time-series predictors \textit {tbats}. In red the final forecast of our HERMES hybrid model \textit {hermes-tbats}. (Top) Time series representing a top fashion trend for females in The United States. (Bottom) Time series representing the horizontal stipes texture fashion trend for females in China}{figure.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \textit  {tbats}, \textit  {hermes-tbats} and \textit  {hermes-tbats-ws} models confusion matrix}}{18}{table.3}\protected@file@percent }
\newlabel{tab:tbatsclass}{{3}{18}{\textit {tbats}, \textit {hermes-tbats} and \textit {hermes-tbats-ws} models confusion matrix}{table.3}{}}
\citation{makridakis2020m4}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Results summary on the 1000 time series and 100 time series Fashion dataset. The MASE average on all our time series is computed. For the two approaches using a neural network, 10 models with different seeds are trained. the mean and the standard deviation of the 10 results are displayed.}}{19}{table.4}\protected@file@percent }
\newlabel{tab:1000metricresults}{{4}{19}{Results summary on the 1000 time series and 100 time series Fashion dataset. The MASE average on all our time series is computed. For the two approaches using a neural network, 10 models with different seeds are trained. the mean and the standard deviation of the 10 results are displayed}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Result for M4 weekly dataset}{19}{subsection.4.4}\protected@file@percent }
\citation{makridakis2020m4}
\citation{makridakis2020m4}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces One of the shortest sequences of the M4 weekly dataset (93 time steps). In order to fit its predictor, the last complete year is duplicated in order to reach a total length of 300 time steps.}}{20}{figure.8}\protected@file@percent }
\newlabel{fig:m4dataset}{{8}{20}{One of the shortest sequences of the M4 weekly dataset (93 time steps). In order to fit its predictor, the last complete year is duplicated in order to reach a total length of 300 time steps}{figure.8}{}}
\citation{hyndman2020package}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{21}{section.5}\protected@file@percent }
\newlabel{sec:conclusion}{{5}{21}{Conclusion}{section.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Results summary on the m4 weekly dataset. For each metric, the average on all our time series is computed. For approaches using a neural network, 10 models are trained with different seeds. The mean and the standard deviation of the 10 results are displayed.}}{22}{table.5}\protected@file@percent }
\newlabel{tab:m4metricresults}{{5}{22}{Results summary on the m4 weekly dataset. For each metric, the average on all our time series is computed. For approaches using a neural network, 10 models are trained with different seeds. The mean and the standard deviation of the 10 results are displayed}{table.5}{}}
\newlabel{sec:m4overview}{{5}{22}{Appendix A. M4 weekly dataset and results}{section*.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces M4 weekly dataset overview. For each category, the number of sequences and the average length are given.}}{23}{table.6}\protected@file@percent }
\newlabel{tab:m4dataset}{{6}{23}{M4 weekly dataset overview. For each category, the number of sequences and the average length are given}{table.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Loss grid search on the Fashion Dataset}{23}{subsection.5.1}\protected@file@percent }
\newlabel{sec:gridsearch}{{5.1}{23}{Loss grid search on the Fashion Dataset}{subsection.5.1}{}}
\newlabel{fig:m4examples:sub1}{{5}{24}{Appendix A. M4 weekly dataset and results}{table.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Examples of time series from the M4 weekly dataset. From Top to Bottom : time series called \textit  {W10} from the \textit  {Other} category, \textit  {W20} from the \textit  {Macro} category and \textit  {W220} from the \textit  {Finance} category.}}{24}{figure.9}\protected@file@percent }
\newlabel{fig:m4examples}{{9}{24}{Examples of time series from the M4 weekly dataset. From Top to Bottom : time series called \textit {W10} from the \textit {Other} category, \textit {W20} from the \textit {Macro} category and \textit {W220} from the \textit {Finance} category}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \textit  {hermes-tbats} forecast examples on the M4 weekly dataset. In green the prediction of the per-time-series predictors \textit  {tbats}. In red the final forecast of our HERMES hybrid model \textit  {hermes-tbats}. (Top) the \textit  {W133} time series, (Bottom) the \textit  {W314} time series.}}{25}{figure.10}\protected@file@percent }
\newlabel{fig:m4pred}{{10}{25}{\textit {hermes-tbats} forecast examples on the M4 weekly dataset. In green the prediction of the per-time-series predictors \textit {tbats}. In red the final forecast of our HERMES hybrid model \textit {hermes-tbats}. (Top) the \textit {W133} time series, (Bottom) the \textit {W314} time series}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces MASE accuray for the \textit  {hermes-tbats-ws} model depending on the loss used during the RNN training. For each loss, 10 models with different seeds have been trained. The mean and the standard deviation are represented with a point and a vertical line.}}{26}{figure.11}\protected@file@percent }
\newlabel{fig:loss_function}{{11}{26}{MASE accuray for the \textit {hermes-tbats-ws} model depending on the loss used during the RNN training. For each loss, 10 models with different seeds have been trained. The mean and the standard deviation are represented with a point and a vertical line}{figure.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Parameters grid search on the M4 weekly Dataset}{26}{subsection.5.2}\protected@file@percent }
\newlabel{sec:m4gridsearch}{{5.2}{26}{Parameters grid search on the M4 weekly Dataset}{subsection.5.2}{}}
\bibdata{hermes_paper_elsevier}
\bibcite{bandara2020lstm}{{1}{2020}{{Bandara et~al.}}{{Bandara, Bergmeir \& Hewamalage}}}
\bibcite{box2015time}{{2}{2015}{{Box et~al.}}{{Box, Jenkins, Reinsel \& Ljung}}}
\bibcite{RePEc:inm:oropre:v:9:y:1961:i:5:p:673-685}{{3}{1961}{{Brown \& Meyer}}{{}}}
\bibcite{douc2014nonlinear}{{4}{2014}{{Douc et~al.}}{{Douc, Moulines \& Stoffer}}}
\bibcite{hochreiter1997long}{{5}{1997}{{Hochreiter \& Schmidhuber}}{{}}}
\bibcite{hyndman2020package}{{6}{2020}{{Hyndman et~al.}}{{Hyndman, Athanasopoulos, Bergmeir, Caceres, Chhay, O'Hara-Wild, Petropoulos, Razbash \& Wang}}}
\bibcite{jianwei2019novel}{{7}{2019}{{Jianwei et~al.}}{{Jianwei, Ye \& Jin}}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces OWA for the \textit  {hermes-tbats} model on the M4 weekly dataset depending on 3 parameters used during the RNN training: Number of moving windows per time series, the batch size and the learning rate. For each parameter, 10 models with different seeds have been trained. The mean and the standard deviation are represented with a point and a vertical. (Top) Result of the HERMES model depending on the number of windows provided per time series to the RNN corrector. (Middle) Result of the HERMES model depending on the size of the batch size. (Bottom) Result of the HERMES model depending on the learning rate of the optimizer.}}{28}{figure.12}\protected@file@percent }
\newlabel{fig:m4parameter}{{12}{28}{OWA for the \textit {hermes-tbats} model on the M4 weekly dataset depending on 3 parameters used during the RNN training: Number of moving windows per time series, the batch size and the learning rate. For each parameter, 10 models with different seeds have been trained. The mean and the standard deviation are represented with a point and a vertical. (Top) Result of the HERMES model depending on the number of windows provided per time series to the RNN corrector. (Middle) Result of the HERMES model depending on the size of the batch size. (Bottom) Result of the HERMES model depending on the learning rate of the optimizer}{figure.12}{}}
\bibcite{li2019enhancing}{{8}{2019}{{Li et~al.}}{{Li, Jin, Xuan, Zhou, Chen, Wang \& Yan}}}
\bibcite{lim2019temporal}{{9}{2019}{{Lim et~al.}}{{Lim, Arik, Loeff \& Pfister}}}
\bibcite{doi:10.1198/jasa.2011.tm09771}{{10}{2011}{{Livera et~al.}}{{Livera, Hyndman \& Snyder}}}
\bibcite{makridakis2018m4}{{11}{2018}{{Makridakis et~al.}}{{Makridakis, Spiliotis \& Assimakopoulos}}}
\bibcite{makridakis2020m4}{{12}{2020}{{Makridakis et~al.}}{{Makridakis, Spiliotis \& Assimakopoulos}}}
\bibcite{martin2020monte}{{13}{2021}{{Martin et~al.}}{{Martin, Ollion, Strub, Le~Corff \& Pietquin}}}
\bibcite{rogersdiffusion}{{14}{1962}{{Rogers}}{{}}}
\bibcite{salinas2020deepar}{{15}{2020}{{Salinas et~al.}}{{Salinas, Flunkert, Gasthaus \& Januschowski}}}
\bibcite{sarkka2013bayesian}{{16}{2013}{{S\"arkk\"a}}{{}}}
\bibcite{8614252}{{17}{2018}{{Siami-Namini et~al.}}{{Siami-Namini, Tavakoli \& Siami~Namin}}}
\bibcite{smyl2020hybrid}{{18}{2020}{{Smyl}}{{}}}
\bibcite{touron2017modeling}{{19}{2017}{{Touron}}{{}}}
\bibcite{touron2019consistency}{{20}{2019}{{Touron}}{{}}}
\bibcite{vaswani2017attention}{{21}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser \& Polosukhin}}}
\bibcite{zhang2003time}{{22}{2003}{{Zhang}}{{}}}
\bibcite{zucchini2017hidden}{{23}{2017}{{Zucchini et~al.}}{{Zucchini, MacDonald \& Langrock}}}
