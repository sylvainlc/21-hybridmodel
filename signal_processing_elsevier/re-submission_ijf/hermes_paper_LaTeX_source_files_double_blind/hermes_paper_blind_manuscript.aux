\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{model5-names}
\citation{makridakis2018}
\citation{sarkka2013,douc2014,zucchini2017}
\citation{Brown1961}
\citation{alysha2011}
\citation{box2015}
\citation{touron2017}
\citation{touron2019}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{hochreiter1997,vaswani2017,siami2018,li2019,lim2019,salinas2020}
\citation{salinas2020}
\citation{li2019}
\citation{lim2019}
\citation{martin2020}
\citation{makridakis2018}
\citation{ma2020}
\citation{ma2020}
\citation{ren2015,chollet2017}
\citation{zhang2003,jianwei2019,bandara2020}
\citation{smyl2020}
\citation{makridakis2020}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A shoes trend of the fashion dataset. In black the main signal and in orange its associated \textit  {fashion-forward} weak signal. The sudden explosion of the influencers signal at the end of 2018 announces the future burst of the trend in the mass market.}}{5}{figure.1}\protected@file@percent }
\newlabel{fig:oneemergingtrend}{{1}{5}{A shoes trend of the fashion dataset. In black the main signal and in orange its associated \textit {fashion-forward} weak signal. The sudden explosion of the influencers signal at the end of 2018 announces the future burst of the trend in the mass market}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Hybrid model with external signals}{5}{section.2}\protected@file@percent }
\newlabel{sec:hybrid}{{2}{5}{Hybrid model with external signals}{section.2}{}}
\citation{smyl2020}
\citation{smyl2020}
\citation{alysha2011}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Per-time-series predictors}{6}{subsection.2.1}\protected@file@percent }
\newlabel{eq:predictors}{{1}{6}{Per-time-series predictors}{equation.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Hermes forecast example on a time series representing the vertical stipes texture fashion trend for females in Brazil. In green the prediction of the TBATS per-time-series predictors. In red the final forecast of our HERMES hybrid model.}}{7}{figure.2}\protected@file@percent }
\newlabel{fig:introexamples}{{2}{7}{Hermes forecast example on a time series representing the vertical stipes texture fashion trend for females in Brazil. In green the prediction of the TBATS per-time-series predictors. In red the final forecast of our HERMES hybrid model}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Error-corrector recurrent model}{7}{subsection.2.2}\protected@file@percent }
\newlabel{eq:nows:full:model}{{2}{8}{Error-corrector recurrent model}{equation.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Weak signal}{8}{subsection.2.3}\protected@file@percent }
\citation{ren2015}
\citation{lin2014}
\newlabel{eq:withws:full:model}{{3}{9}{Weak signal}{equation.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Fashion dataset with external weak signals}{9}{section.3}\protected@file@percent }
\newlabel{sec:dataset}{{3}{9}{Fashion dataset with external weak signals}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Translate fashion to data}{9}{subsection.3.1}\protected@file@percent }
\newlabel{sec:dataset:a}{{3.1}{9}{Translate fashion to data}{subsection.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Architecture of the hybrid model with weak signals. The proposed framework can be decomposed in 5 steps: i) provide a time series. ii) (a) fit a first statistical model with the provided time series, (b) compute a first prediction and (c) preprocess the time series for the Global RNN. iii) If available, external signals can be added as part of the RNN input. iv) With a pre-trained RNN, compute a correction of the first statistical prediction. v) Compute the final forecast by adding the first time series prediction and the RNN correction.}}{10}{figure.3}\protected@file@percent }
\newlabel{fig:architecture}{{3}{10}{Architecture of the hybrid model with weak signals. The proposed framework can be decomposed in 5 steps: i) provide a time series. ii) (a) fit a first statistical model with the provided time series, (b) compute a first prediction and (c) preprocess the time series for the Global RNN. iii) If available, external signals can be added as part of the RNN input. iv) With a pre-trained RNN, compute a correction of the first statistical prediction. v) Compute the final forecast by adding the first time series prediction and the RNN correction}{figure.3}{}}
\citation{chollet2017}
\citation{russakovsky2014}
\citation{cleveland1990}
\citation{rogers1962}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Fashion time series}{12}{subsection.3.2}\protected@file@percent }
\newlabel{sec:dataset:b}{{3.2}{12}{Fashion time series}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Weak signal}{12}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Example of difference between the raw sequence and the normalized one for the Jersey top fashion trend for females in China. In this example, we normalize by the deseasonalized global top fashion trend for females in China. (Top) Time series representing the raw signal of the Jersey top fashion trend for females in China. (Bottom) Time series representing the normalized signal of the Jersey top fashion trend for females in China.}}{13}{figure.4}\protected@file@percent }
\newlabel{fig:normalization}{{4}{13}{Example of difference between the raw sequence and the normalized one for the Jersey top fashion trend for females in China. In this example, we normalize by the deseasonalized global top fashion trend for females in China. (Top) Time series representing the raw signal of the Jersey top fashion trend for females in China. (Bottom) Time series representing the normalized signal of the Jersey top fashion trend for females in China}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Fashion dataset}{14}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental results}{14}{section.4}\protected@file@percent }
\newlabel{sec:exp}{{4}{14}{Experimental results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Training}{14}{subsection.4.1}\protected@file@percent }
\citation{smyl2020}
\citation{alysha2011}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Fashion time series overview. For each couple geozone/category, the table gives the number of trends (Female/Male).}}{15}{table.1}\protected@file@percent }
\newlabel{tab:fashiondataset}{{1}{15}{Fashion time series overview. For each couple geozone/category, the table gives the number of trends (Female/Male)}{table.1}{}}
\citation{hyndman2020}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Temporal split for our training process. The three first years define our training set. The fourth year is used as our eval set and the final year is reserved for the test set.}}{16}{figure.5}\protected@file@percent }
\newlabel{fig:train_eval_test_set}{{5}{16}{Temporal split for our training process. The three first years define our training set. The fourth year is used as our eval set and the final year is reserved for the test set}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Benchmarks, hybrid models and Metrics}{16}{subsection.4.2}\protected@file@percent }
\newlabel{sec:fashiontraining}{{4.2}{16}{Benchmarks, hybrid models and Metrics}{subsection.4.2}{}}
\citation{smyl2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Result for the Fashion dataset}{18}{subsection.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Results summary on the 10000ts Fashion dataset. For each metric, the average on all our time series is computed. For approaches using neural networks, 10 models are trained with different seeds. The mean and the standard deviation of the 10 results are displayed.}}{19}{table.2}\protected@file@percent }
\newlabel{tab:metricresults}{{2}{19}{Results summary on the 10000ts Fashion dataset. For each metric, the average on all our time series is computed. For approaches using neural networks, 10 models are trained with different seeds. The mean and the standard deviation of the 10 results are displayed}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \textit  {tbats}, \textit  {hermes-tbats} and \textit  {hermes-tbats-ws} models confusion matrix}}{19}{table.3}\protected@file@percent }
\newlabel{tab:tbatsclass}{{3}{19}{\textit {tbats}, \textit {hermes-tbats} and \textit {hermes-tbats-ws} models confusion matrix}{table.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textit  {hermes-tbats} forecast examples. In green the prediction of the per-time-series predictors \textit  {tbats}. In red the final forecast of our HERMES hybrid model \textit  {hermes-tbats}. (Top) Time series representing a top fashion trend for females in The United States. (Bottom) Time series representing the horizontal stipes texture fashion trend for females in China.}}{20}{figure.6}\protected@file@percent }
\newlabel{fig:examples}{{6}{20}{\textit {hermes-tbats} forecast examples. In green the prediction of the per-time-series predictors \textit {tbats}. In red the final forecast of our HERMES hybrid model \textit {hermes-tbats}. (Top) Time series representing a top fashion trend for females in The United States. (Bottom) Time series representing the horizontal stipes texture fashion trend for females in China}{figure.6}{}}
\citation{makridakis2020}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Results summary on the 1000 time series and 100 time series Fashion dataset. The MASE average on all the time series is computed. For the two approaches using a neural network, 10 models with different seeds are trained. the mean and the standard deviation of the 10 results are displayed.}}{21}{table.4}\protected@file@percent }
\newlabel{tab:1000metricresults}{{4}{21}{Results summary on the 1000 time series and 100 time series Fashion dataset. The MASE average on all the time series is computed. For the two approaches using a neural network, 10 models with different seeds are trained. the mean and the standard deviation of the 10 results are displayed}{table.4}{}}
\citation{makridakis2020}
\citation{smyl2020}
\citation{darin2020}
\citation{petropoulos2020}
\citation{pawlikowski2020}
\citation{montero2020}
\citation{makridakis2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Result for M4 weekly dataset}{22}{subsection.4.4}\protected@file@percent }
\newlabel{sec:m4result}{{4.4}{22}{Result for M4 weekly dataset}{subsection.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces One of the shortest sequences of the M4 weekly dataset (93 time steps). In order to fit its predictor, the last complete year of the train set is duplicated in order to reach a total length of 300 time steps.}}{23}{figure.7}\protected@file@percent }
\newlabel{fig:m4dataset}{{7}{23}{One of the shortest sequences of the M4 weekly dataset (93 time steps). In order to fit its predictor, the last complete year of the train set is duplicated in order to reach a total length of 300 time steps}{figure.7}{}}
\citation{smyl2020}
\citation{smyl2020}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{24}{section.5}\protected@file@percent }
\newlabel{sec:conclusion}{{5}{24}{Conclusion}{section.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Results summary on the m4 weekly dataset. For each metric, the average on all our time series is computed. For approaches using a neural network, 10 models are trained with different seeds. The mean and the standard deviation of the 10 results are displayed.}}{25}{table.5}\protected@file@percent }
\newlabel{tab:m4metricresults}{{5}{25}{Results summary on the m4 weekly dataset. For each metric, the average on all our time series is computed. For approaches using a neural network, 10 models are trained with different seeds. The mean and the standard deviation of the 10 results are displayed}{table.5}{}}
\@writefile{toc}{\let\numberline\tmptocnumberline}
\@writefile{toc}{\contentsline {section}{\numberline {Appendix \nobreakspace  {}A}M4 weekly dataset, Ensembling training and results}{26}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {Appendix \nobreakspace  {}A.1}M4 weekly dataset}{26}{subsection.A.1}\protected@file@percent }
\newlabel{sec:m4overview}{{Appendix \nobreakspace  {}A.1}{26}{M4 weekly dataset}{subsection.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {Appendix \nobreakspace  {}A.2}M4 accuracy metrics}{26}{subsection.A.2}\protected@file@percent }
\newlabel{sec:m4metric}{{Appendix \nobreakspace  {}A.2}{26}{M4 accuracy metrics}{subsection.A.2}{}}
\newlabel{fig:m4examples:sub1}{{Appendix \nobreakspace  {}A.1}{27}{M4 weekly dataset}{table.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.8}{\ignorespaces Examples of time series from the M4 weekly dataset. From Top to Bottom : time series called \textit  {W10} from the \textit  {Other} category, \textit  {W20} from the \textit  {Macro} category and \textit  {W220} from the \textit  {Finance} category.}}{27}{figure.8}\protected@file@percent }
\newlabel{fig:m4examples}{{A.8}{27}{Examples of time series from the M4 weekly dataset. From Top to Bottom : time series called \textit {W10} from the \textit {Other} category, \textit {W20} from the \textit {Macro} category and \textit {W220} from the \textit {Finance} category}{figure.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.6}{\ignorespaces M4 weekly dataset overview. For each category, the number of sequences and the average length are given.}}{28}{table.6}\protected@file@percent }
\newlabel{tab:m4dataset}{{A.6}{28}{M4 weekly dataset overview. For each category, the number of sequences and the average length are given}{table.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.9}{\ignorespaces forecast examples of HERMES variations on 2 time series of the M4 weekly dataset. At the top, the \textit  {W133} time series is displayed with the prediction of the per-time-series predictor \textit  {thetam} (green) and the final forecast of the HERMES hybrid model \textit  {hermes-thetam} (red). At the bottom, the \textit  {W262} time series is represented with the corresponding prediction of the per-time-series predictors \textit  {ets-add} (green) and the HERMES correction of \textit  {hermes-ets-add} (red).}}{28}{figure.9}\protected@file@percent }
\newlabel{fig:m4pred}{{A.9}{28}{forecast examples of HERMES variations on 2 time series of the M4 weekly dataset. At the top, the \textit {W133} time series is displayed with the prediction of the per-time-series predictor \textit {thetam} (green) and the final forecast of the HERMES hybrid model \textit {hermes-thetam} (red). At the bottom, the \textit {W262} time series is represented with the corresponding prediction of the per-time-series predictors \textit {ets-add} (green) and the HERMES correction of \textit {hermes-ets-add} (red)}{figure.9}{}}
\citation{montero2020}
\@writefile{toc}{\contentsline {subsection}{\numberline {Appendix \nobreakspace  {}A.3}FFORMA ensembling with HERMES variations}{29}{subsection.A.3}\protected@file@percent }
\newlabel{sec:fforma-hermes}{{Appendix \nobreakspace  {}A.3}{29}{FFORMA ensembling with HERMES variations}{subsection.A.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.7}{\ignorespaces Results summary on the m4 weekly dataset of the HERMES variations. For each metric, the average on all the time series is computed. For approaches using a neural network, 10 models are trained with different seeds. The mean and the standard deviation of the 10 results are displayed. For the statistical models \textit  {ets-add}, \textit  {ets-mul} and \textit  {thetam}, the Python package {\texttt  {statsmodels}} is used. The Python package \texttt  {tbats} is used for the \textit  {tbats} approach.}}{30}{table.7}\protected@file@percent }
\newlabel{tab:m4hermesvariationsresults}{{A.7}{30}{Results summary on the m4 weekly dataset of the HERMES variations. For each metric, the average on all the time series is computed. For approaches using a neural network, 10 models are trained with different seeds. The mean and the standard deviation of the 10 results are displayed. For the statistical models \textit {ets-add}, \textit {ets-mul} and \textit {thetam}, the Python package {\texttt {statsmodels}} is used. The Python package \texttt {tbats} is used for the \textit {tbats} approach}{table.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {Appendix \nobreakspace  {}A.4}M4 weekly dataset results}{30}{subsection.A.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {Appendix \nobreakspace  {}B}Training parameters and loss}{30}{appendix.B}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {Appendix \nobreakspace  {}B.1}Loss grid search on the Fashion Dataset}{30}{subsection.B.1}\protected@file@percent }
\newlabel{sec:heuritechgridsearch}{{Appendix \nobreakspace  {}B.1}{30}{Loss grid search on the Fashion Dataset}{subsection.B.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.10}{\ignorespaces MASE accuray for the \textit  {hermes-tbats-ws} model depending on the loss used during the RNN training. For each loss, 10 models with different seeds have been trained. The mean and the standard deviation are represented with a point and a vertical line.}}{31}{figure.10}\protected@file@percent }
\newlabel{fig:loss_function}{{B.10}{31}{MASE accuray for the \textit {hermes-tbats-ws} model depending on the loss used during the RNN training. For each loss, 10 models with different seeds have been trained. The mean and the standard deviation are represented with a point and a vertical line}{figure.10}{}}
\bibdata{hermes_paper_elsevier}
\bibcite{bandara2020}{{1}{2020}{{Bandara et~al.}}{{Bandara, Bergmeir \& Hewamalage}}}
\bibcite{box2015}{{2}{2015}{{Box et~al.}}{{Box, Jenkins, Reinsel \& Ljung}}}
\bibcite{Brown1961}{{3}{1961}{{Brown \& Meyer}}{{}}}
\bibcite{chollet2017}{{4}{2017}{{Chollet}}{{}}}
\bibcite{cleveland1990}{{5}{1990}{{Cleveland et~al.}}{{Cleveland, Cleveland, McRae \& Terpenning}}}
\bibcite{darin2020}{{6}{2020}{{Darin \& Stellwagen}}{{}}}
\bibcite{douc2014}{{7}{2014}{{Douc et~al.}}{{Douc, Moulines \& Stoffer}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {Appendix \nobreakspace  {}B.2}Parameters grid search on the M4 weekly Dataset}{32}{subsection.B.2}\protected@file@percent }
\newlabel{sec:m4gridsearch}{{Appendix \nobreakspace  {}B.2}{32}{Parameters grid search on the M4 weekly Dataset}{subsection.B.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {B.11}{\ignorespaces OWA for the \textit  {hermes-tbats} model on the eval set of the M4 weekly dataset. 5 hyperparameters used during the RNN training are tested: the number of moving windows per time series (top left), the learning rate (top right), the batch size (middle), the window size for the RNN input (bottom left) and the dimension of the LSTM layers output (bottom right). For each parameter, 10 models with different seeds have been trained. The mean and the standard deviation of the OWA on the eval set are represented with a point and a vertical line.}}{33}{figure.11}\protected@file@percent }
\newlabel{fig:m4parameter}{{B.11}{33}{OWA for the \textit {hermes-tbats} model on the eval set of the M4 weekly dataset. 5 hyperparameters used during the RNN training are tested: the number of moving windows per time series (top left), the learning rate (top right), the batch size (middle), the window size for the RNN input (bottom left) and the dimension of the LSTM layers output (bottom right). For each parameter, 10 models with different seeds have been trained. The mean and the standard deviation of the OWA on the eval set are represented with a point and a vertical line}{figure.11}{}}
\bibcite{hochreiter1997}{{8}{1997}{{Hochreiter \& Schmidhuber}}{{}}}
\bibcite{hyndman2020}{{9}{2020}{{Hyndman et~al.}}{{Hyndman, Athanasopoulos, Bergmeir, Caceres, Chhay, O'Hara-Wild, Petropoulos, Razbash \& Wang}}}
\bibcite{jianwei2019}{{10}{2019}{{Jianwei et~al.}}{{Jianwei, Ye \& Jin}}}
\bibcite{li2019}{{11}{2019}{{Li et~al.}}{{Li, Jin, Xuan, Zhou, Chen, Wang \& Yan}}}
\bibcite{lim2019}{{12}{2019}{{Lim et~al.}}{{Lim, Arik, Loeff \& Pfister}}}
\bibcite{lin2014}{{13}{2014}{{Lin et~al.}}{{Lin, Maire, Belongie, Bourdev, Girshick, Hays, Perona, Ramanan, Zitnick \& Doll√°r}}}
\bibcite{alysha2011}{{14}{2011}{{Livera et~al.}}{{Livera, Hyndman \& Snyder}}}
\bibcite{ma2020}{{15}{2020}{{Ma et~al.}}{{Ma, Ding, Yang, Liao, Wong \& Chua}}}
\bibcite{makridakis2018}{{16}{2018}{{Makridakis et~al.}}{{Makridakis, Spiliotis \& Assimakopoulos}}}
\bibcite{makridakis2020}{{17}{2020}{{Makridakis et~al.}}{{Makridakis, Spiliotis \& Assimakopoulos}}}
\bibcite{martin2020}{{18}{2021}{{Martin et~al.}}{{Martin, Ollion, Strub, Le~Corff \& Pietquin}}}
\bibcite{montero2020}{{19}{2020}{{Montero-Manso et~al.}}{{Montero-Manso, Athanasopoulos, Hyndman \& Talagala}}}
\bibcite{pawlikowski2020}{{20}{2020}{{Pawlikowski \& Chorowska}}{{}}}
\bibcite{petropoulos2020}{{21}{2020}{{Petropoulos \& Svetunkov}}{{}}}
\bibcite{ren2015}{{22}{2015}{{Ren et~al.}}{{Ren, He, Girshick \& Sun}}}
\bibcite{rogers1962}{{23}{1962}{{Rogers}}{{}}}
\bibcite{russakovsky2014}{{24}{2014}{{Russakovsky et~al.}}{{Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, Berg \& Fei-Fei}}}
\bibcite{salinas2020}{{25}{2020}{{Salinas et~al.}}{{Salinas, Flunkert, Gasthaus \& Januschowski}}}
\bibcite{sarkka2013}{{26}{2013}{{S{\"a}rkk{\"a}}}{{}}}
\bibcite{siami2018}{{27}{2018}{{Siami-Namini et~al.}}{{Siami-Namini, Tavakoli \& Siami~Namin}}}
\bibcite{smyl2020}{{28}{2020}{{Smyl}}{{}}}
\bibcite{touron2017}{{29}{2017}{{Touron}}{{}}}
\bibcite{touron2019}{{30}{2019}{{Touron}}{{}}}
\bibcite{vaswani2017}{{31}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser \& Polosukhin}}}
\bibcite{zhang2003}{{32}{2003}{{Zhang}}{{}}}
\bibcite{zucchini2017}{{33}{2017}{{Zucchini et~al.}}{{Zucchini, MacDonald \& Langrock}}}
